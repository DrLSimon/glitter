<div>
    <h2>Overview</h2>
    <p>
        This assignment consists of two parts. In the first, you'll get to
        implement Monte Carlo sampling techniques that warp an initially
        uniform distribution into a number of different target distributions.
        Note that all work you do in this assignment will serve as building
        blocks in later assignments when we apply Monte Carlo integration to
        render images. In particular, assignments 4 and 5 will build on this
        functionality to create material models and light sources suitable for
        photorealistic rendering.
    </p>
    <p>
        In the second part, you will implement two very basic rendering
        algorithms: the first algorithm is fully deterministic and renders a
        scene illuminated by a single point lights. The second is a
        stochastic rendering algorithm known as <em>Ambient Occlusion</em>,
        which assumes that the scene is uniformly illuminated from all
        directions.
    </p>
    <p>
        As usual, begin by importing the latest base code updates into your
        repository by running 
    </p>
<pre class="prettyprint lang-bash">
git pull https://github.com/wjakob/nori
</pre>
    <p>
        If there were any concurrent changes to the same file, you
        may have to perform a <em>merge</em> (see the git tutorials under
        "Preliminaries" for more information). 
    </p>


    <h2>Part 1: Monte Carlo Sampling (60 pts)</h2>

	<p>
        In this exercise you will generate sample points on various domains:
        the plane, disks, spheres, and hemispheres. The base code has been
        extended with an interactive visualization and testing tool to make
        working with point sets as intuitive as possible. 
	</p>

	<p>
        After pulling the latest updates and recompiling,
        you should see an executable named <tt>warptest</tt>. Run this
        executable to launch the interactive warping tool, which allows you to
        visualize the behavior of different warping functions given a range of
        input point sets (independent, grid, and stratified). Up to now, we
        only discussed uniform random variables which correspond to the
        "independent" type, and you need not concern yourself with the others
        for now.
	</p>
	<p>
        Part 1 is split into several subsections; in each case, you are asked
        to implement a distribution function and a matching sample warping
        scheme It is crucial that both are <em>consistent</em> with respect to
        each other (i.e. that warped samples have exactly the distribution
        described by the density function). Significant errors can arise if
        inconsistent warpings are used for Monte Carlo integration. The
        <tt>warptest</tt> tool provided by us implements a \(\chi^2\) test to
        ensure that this consistency requirement is indeed satisfied.
	</p>

	<div class="alert alert-danger" role="alert">
        Note that passing the test does not generally imply that your
        implementation is correct—for instance, the test may not have enough
        "evidence" to generate a failure, or potentially the warping function
        and the density function are both incorrect in the same manner. Use
        your judgment and don't rely on this test alone.
	</div>

	<div class="row" style="margin: 30px">
		<div class="col-md-3">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/warp-square-points.png"><img src="images/warp-square-points.png"/></a>
				<div class="caption">
					The input point set (stratified samples passed through a "no-op" warp function)
				</div>
			</div>
		</div>
		<div class="col-md-3">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/warp-square-chi2.png"><img src="images/warp-square-chi2.png"/></a>
				<div class="caption">
					This point set passed the test for uniformity.
				</div>
			</div>
		</div>
		<div class="col-md-3">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/warp-disk-points.png"><img src="images/warp-disk-points.png"/></a>
				<div class="caption">
					A more interesting case that you will implement
					(with a grid visualization of the mapping)
				</div>
			</div>
		</div>
		<div class="col-md-3">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/warp-disk-chi2.png"><img src="images/warp-disk-chi2.png"/></a>
				<div class="caption">
					This warping passed the tests as well.
				</div>
			</div>
		</div>
	</div>


	<h3>Part 1.1. Sample Warping <em>(30 points)</em></h3>
	<p>
        Implement the missing functions in <code>class Warp</code> found in
        <tt>src/warp.cpp</tt>. This class consists of various warp methods that
        take as input a 2D point \((s, t) \in [0, 1) \times [0, 1) \) and
        return the warped 2D (or 3D) point in the new domain. Each method is
        accompanied by another method that returns the probability density with
        which a sample was picked. Our default implementations all throw an
        exception, which produces an error message in the graphical user
        interface of <tt>warptest</tt>. The slides on the course website
        provide a number of useful recipes for warping samples and computing
        the densities, and the PBRT textbook also contains considerable
        information on this topic that you should feel free to use.
	</p>

	<ul>
		<li>
			<h4><code>Warp::squareToTent</code> and <code>Warp::squareToTentPdf</code> <em> (15 Points)</em></h4>
			<p>
                Implement a method that transforms uniformly distributed 2D
                points on the unit square into the 2D "tent" distribution,
                which has the following form:
				\[
                p(x, y)=p_1(x)\,p_1(y)\quad\text{and}\quad
                p_1(t) = \begin{cases}
                1-|x|, & -1\le x\le 1\\
                0,&\text{otherwise}\\
                \end{cases}
				\]
			</p>
            Note that this distribution is composed of two independent 1D
            distributions, which makes this task considerably easier. Follow
            the "recipe" discussed in class:
            <ol>
                <li>Compute the CDF \(P_1(t)\) of the 1D distribution \(p_1(t)\)</li>
                <li>Derive the inverse \(P_1^{-1}(t)\)</li>
                <li>Map a random variable \(\xi\) through the inverse \(P_1^{-1}(t)\) from the previous step</li>
            </ol>
            Show the details of these steps in your report (either using TeX, or by taking
            a photograph of the derivation and embedding the image)
		</li>
		<li>
			<h4><code>Warp::squareToUniformDisk</code> and <code>Warp::squareToUniformDiskPdf</code> <em> (10 Points)</em></h4>
			<p>
				Implement a method that transforms uniformly distributed 2D
				points on the unit square into uniformly distributed points on
				a planar <em>disk</em> with radius 1 centered at the origin. Next,
				implement a probability density function that matches your
				warping scheme.
			</p>
		</li>
		<li>
			<h4><code>Warp::squareToUniformSphere</code> and <code>Warp::squareToUniformSpherePdf</code> <em> (10 Points)</em></h4>
			<p>
				Implement a method that transforms uniformly distributed 2D
				points on the unit square into uniformly distributed points on
				the <em>unit sphere</em> centered at the origin. Implement a matching
				probability density function.
			</p>
		</li>
		<li>
			<h4><code>Warp::squareToUniformHemisphere</code> and <code>Warp::squareToUniformHemispherePdf</code> <em> (5 Points)</em></h4>
			<p>
				Implement a method that transforms uniformly distributed 2D
				points on the unit square into uniformly distributed points on
				the <em>unit hemisphere</em> centered at the origin and oriented in
				direction \((0, 0, 1)\). Add a matching probability density
				function.
			</p>
		</li>
		<li>
			<h4><code>Warp::squareToCosineHemisphere</code> and <code>Warp::squareToCosineHemispherePdf</code> <em> (5 Points)</em></h4>
			<p>
				Transform your 2D point to a point distributed on the unit
				hemisphere with a cosine density function
				\[
				p(\theta)=\frac{\cos\theta}{\pi},
				\]
				where \(\theta\) is the
				angle between a point on the hemisphere and the north pole.
			</p>
		</li>
	</ul>
	<h3>Part 1.2: Validation <em>(30 points)</em></h3>
	<p>
		Pass the \(\chi^2\) test for each one of the above sampling techniques
        and include screen shots in your report.
	</p>

	<h2>Part 2: Two simple rendering algorithms <em>(40 points)</em></h2>
    <p>
        In this part of the homework, you'll implement two basic rendering
        algorithms that set the stage for fancier methods investigated later
        in the course. For now, both of the methods assume that the object is
        composed of a simple white diffuse material that reflects light
        uniformly into all directions.
    </p>
	<div class="row" style="margin: 30px">
		<div class="col-md-3">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/ajax-simple.png"><img src="images/ajax-simple.png"/></a>
				<div class="caption">
					The Ajax bust illuminated by a point light source.
				</div>
			</div>
		</div>
		<div class="col-md-3">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/ajax-ao.png"><img src="images/ajax-ao.png"/></a>
				<div class="caption">
					The Ajax bust rendered using Ambient Occlusion.
				</div>
			</div>
		</div>
	</div>
	<h3>Part 2.2: Point lights <em>(20 points)</em></h3>
	<p>
        The updated base code includes a new scene <code>scenes/pa3/ajax-simple.xml</code> that instantiates a (currently nonexistent) integrator/rendering algorithm named <code>simple</code>, which simulates a single point
        light source located at a 3D position <tt>position</tt>, and which emits an amount of
        energy given by the parameter <tt>energy</tt>.
    </p>
<pre class="prettyprint lang-xml">
&lt;!-- An excerpt from scenes/pa3/ajax-simple.xml: --&gt;
&lt;integrator type="simple"&gt;
    &lt;point name="position" value="-20, 40, 20"/&gt;
    &lt;color name="energy" value="3000, 3000, 3000"/&gt;
&lt;/integrator&gt;
</pre>
    <p>
    Your first task will be to create a new <code>Integrator</code> that accepts
    these parameters. This should be fairly reminiscent of the <code>normal</code>
    integrator from Assignment 1. Take a look at the <code>PropertyList</code> class, which
    should be used to extract the two parameters.
    </p>
    
    <p>
    Let \(\mathbf{p}\) and \(\mathbf{\Phi}\) denote the position and energy of
    the light source, and suppose that \(\mathbf{x}\) is the point being
    rendered. Then this integrator should compute the quantity
    </p>
\[
L(\mathbf{x})=\frac{\Phi}{4\pi^2} \frac{\mathrm{max}(0, \cos\theta)}{\|\mathbf{x}-\mathbf{p}\|^2} V(\mathbf{x}\leftrightarrow\mathbf{p})
\]
    <p>
    where \(\theta\) is the angle between the direction from \(\mathbf{x}\) to \(\mathbf{p}\) and the shading surface normal
    (available in <code>Intersection::shFrame::n</code>) at \(\mathbf{x}\)
    and
    </p>
    \[
    V(\mathbf{x}\leftrightarrow\mathbf{p}):=\begin{cases}
    1,&\text{if $\mathbf{x}$ and $\mathbf{p}$ are mutually visible}\\
    0,&\text{otherwise}
    \end{cases}
    \]
    <p>
    is the visibility function, which can be implemented using a <em>shadow ray</em> query.
    Intersecting a shadow ray against the scene is generally cheaper since it suffices
    to check whether an intersection exists rather than having to find the closest one.
    </p>

    <p>
    Implement the <code>simple</code> integrator according to this specification and
    render the scene <code>scenes/pa3/ajax-simple.xml</code>.
    Include a comparison against our <a href="https://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2017/03/13/ajax-simple.exr">reference image</a> in your report.
	</p>

	<h3>Part 2.1: Ambient occlusion <em>(20 points)</em></h3>
    <p>
        Ambient occlusion is rendering technique which assumes that a (diffuse)
        surface receives uniform illumination from all directions (similar to
        the conditions inside a <a
        href="https://en.wikipedia.org/wiki/Lightbox#/media/File:DIY_Lightbox.jpg">light
    box</a>), and that visibility is the only effect that matters. Some surface positions
will receive less light than others since they are occluded, hence they will look darker.
Formally, the quantity computed by ambient occlusion is defined as
    </p>
\[
L(\mathbf{x})=\int_{H^2(\mathbf{x})}V(\mathbf{x}, \mathbf{x}+\alpha\omega)\,\frac{\cos\theta}{\pi}\,\mathrm{d}\omega
\]
<p>
which is an integral over the upper hemisphere centered at the point
\(\mathbf{x}\). The variable \(\theta\) refers to the angle between the direction \(\omega\)
and the shading normal at \(\mathbf{x}\). The ad-hoc variable \(\alpha\) adjusts
how far-reaching the effects of occlusion are.
</p>
<p>
Note that this situation—sampling points on the hemisphere with a cosine weight—exactly corresponds
to one of the warping functions you implemented in part 1, specifically <code>squareToCosineHemisphere</code>. Use this function to sample a point on the hemisphere and then check for visibility
using a shadow ray query. You can assume that occlusion is a global effect (i.e. \(\alpha=\infty\)).
</p>

<p>
One potential gotcha is that the samples produced by <code>squareToCosineHemisphere</code> lie in the reference hemisphere and need to be oriented according to the surface at \(\mathbf{x}\). Take a look at the <code>Frame</code> class, which is intended to facilitate this.
</p>

<p>
    Implement the ambient occlusion (<code>ao</code>) integrator and render the scene <code>scenes/ajax/ajax-ao.xml</code>.
        Include a comparison against our <a href="https://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2017/03/13/ajax-ao.exr">reference image</a> in your report.

    <h2> Hacker Points: Hierarchical Sample Warping <em>(20 points)</em></h2>

<div class="alert alert-info" role="alert"><b>Disclaimer</b>: Hacker points are “underpriced” bonus points 
for the daring few. Sometimes you might be required to implement something that was not taught in class and 
you might have to do some research and creative thinking. Hacker Points are awarded only to students who 
implemented all of the remaining assignment, and they are added to the final score.
</div>

	<div class="row" style="margin: 30px">
		<div class="row-centered">
		<div class="col-md-3 col-centered">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/envmap-1.jpg"><img src="images/envmap-1.jpg"/></a>
				<div class="caption">
					A lightprobe by <a href="http://dativ.at/lightprobes/">Bernhard Vogl</a>.
				</div>
			</div>
		</div>
		<div class="col-md-3 col-centered">
			<div class="thumbnail" style="margin: 10px">
				<a class="fancybox" href="images/envmap-2.jpg"><img src="images/envmap-2.jpg"/></a>
				<div class="caption">
					A simple scene rendered with this lightprobe.
				</div>
			</div>
		</div>
		</div>
	</div>

	<p> 
		The goal of this exercise is to implement hierarchical sample warping,
		which will come in very handy if you decide you'll want to later add
		<a href="http://ict.usc.edu/pubs/Image-Based%20Lighting.pdf">Image Based Lighting</a> (IBL)
		to your renderer for the final project. Lighting with captured
		real-world lighting conditions ("<em>lightprobes</em>") is a
		relatively cheap and simple way to add considerable realism to a
		rendering. Note that we don't expect you to add the rendering-related
		parts yet; this exercise is just about the warping scheme.
	</p>
	<p>
		You can assume that a non-negative grayscale image is given as an
		input (this image records the per-pixel luminance values of an IBL
		light probe). The image is interpreted as a probability density of
		samples distributed on a 2D rectangular domain, hence the goal is to
		generate samples proportionally to these luminance values.
		Strictly speaking, the image is not really a density function because
		it does not integrate to one—we simply apply a normalization factor
		to circumvent this problem.
	</p>

	<div>
		<p>
			Your implementation of Hierarchical Sample Warping with the help of mip mapping should work as follows: 
			<ul>
				<li> <em>Input</em>: an EXR image with \(2^M \times 2^M \) pixels and a 2D sample uniformly distributed on the unit square.</li>
				<li> Compute the luminance of that image and normalize it. </li>
				<li> Build and store the Mipmap for that image. </li>
				<li> Starting at the top level's children (\(2 \times 2\) px) compute the sum of the luminance values for the two top quadrants and the two bottom quadrants \((80\%,20\%\) in the example figure below).</li>
				<li> Warp the input domain of the sample vertically such that the area of the domains agrees with the ratio of the luminance values. </li>
				<li> Repeat the above two steps independently for the top and bottom row. </li>
				<li> Repeat this process for the quadrant where your sample fell in, until the bottom level of your mipmap is reached (full resolution).</li>
				<li> <em>Output</em>: a sample distributed according to the density function defined by the image (in \(O(M)\)).</li>
			</ul>
			
		</p>
	</div>
	<div class="row-centered">
		<div class="thumbnail col-md-5 col-centered">
			<a class="fancybox" href="images/hsw.png"><img src="images/hsw.png"/></a>
			<div class="caption">
				Left: Uniformly distributed 2D input points. Middle: 2x2 image which defines the sampling density. Right: Hierarchical Sample Warping result applied on the uniformly distributed input points.
			</div>
		</div>
	</div>
	
	<h4> What to submit </h4>
	<p>
		<ul>
			<li> A correct implementation of the hierarchical sample warping algorithm described above. </li> 
			<li> A modified version of the <tt>warptest</tt> user interface that visualizes your generated points sets.</li> 
			<li> Two screen shots of the \(\chi^2\)-test (with many input points), which verifies that your warping function and density function are mutually consistent.
				The first should be with a simple \(2 \times 2\) pixel image as shown above, and for the second, you can choose any
				light probe you like (<a href="http://gl.ict.usc.edu/Data/HighResProbes/">Paul Debevec</a> and
				<a href="http://dativ.at/lightprobes/">Bernhard Vogl</a> provide many light probes free of charge).
				Feel free to change the resolution of your input image to a power of 2 (at least \(512 \times 512\)) using a tool of your choice.
			</li>
		</ul>
	</p>
</div>
